{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENVIRONMENT 1\n",
    "# Testbed - variables\n",
    "num_action = 10\n",
    "num_seed = 1\n",
    "num_runs = 500  # number of simulation runs\n",
    "steps = 1000  # number of steps in each run\n",
    "epsilons= list(np.linspace(0,0.4,5)) + [1] #range of epsilons is between 0-0.4 ==> why dont we go until 1 in incremental steps? \n",
    "\n",
    "# set up the random seed\n",
    "np.random.seed(num_seed)\n",
    " \n",
    "epsilons_list = []\n",
    "total_actions= [0 for x in range(num_action)]\n",
    "# init the environment\n",
    "env = GamblingRoom(k=num_action)\n",
    "for epsilon in epsilons:\n",
    "    best_action = np.argmax([slot_machine.mu for slot_machine in env.r_machines]) # best action in a gambling machine\n",
    "\n",
    "    # delete the wrap\n",
    "    env = env.unwrapped\n",
    "    environment_avg = []\n",
    "    best_picked_run = []\n",
    "\n",
    "    avg_rewards= []\n",
    "    most_frequent = [0 for x in range(num_action)]\n",
    "\n",
    "    # run multiple simulations\n",
    "    for i_run in range(num_runs): \n",
    "        rewards= []\n",
    "        best_picked_step = []\n",
    "        # init the epsilon-greedy RL agent \n",
    "        agent = EplisonGreedyAgent(num_action, epsilon)\n",
    "        # in each simulation run, loop the action selection\n",
    "        # save the result variables you need\n",
    "        for step in range(steps):\n",
    "            action = agent.select_action()\n",
    "            most_frequent[action] =  most_frequent[action] +1\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            agent.update_parameters(action, reward)\n",
    "            condition = action ==best_action\n",
    "            best_picked_step.append(condition)\n",
    "\n",
    "        best_picked_run.append(best_picked_step)\n",
    "\n",
    "        avg_rewards.append(rewards)\n",
    "       # optimal_action = np.argmax(agent.Qvalues)\n",
    "\n",
    "    #environment_avg.append(avg_rewards)\n",
    "\n",
    "    #average the array\n",
    "    ave_array = np.mean(np.array(avg_rewards), axis=0)\n",
    "\n",
    "    # The percentage of times that the optimal action was picked at each episode over the whole runs\n",
    "    best_picked_run = np.mean(best_picked_run, axis = 0)\n",
    "    epsilons_list.append((epsilon,ave_array,most_frequent,best_action,best_picked_run))\n",
    "    epsilons_list_env1 = epsilons_list\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENVIRONMENT 2\n",
    "# Testbed - variables\n",
    "num_action = 10\n",
    "num_seed = 2\n",
    "num_runs = 500  # number of simulation runs\n",
    "steps = 1000  # number of steps in each run\n",
    "epsilons= list(np.linspace(0,0.4,5)) + [1]\n",
    "\n",
    "# set up the random seed\n",
    "np.random.seed(num_seed)\n",
    " \n",
    "epsilons_list = []\n",
    "total_actions= [0 for x in range(num_action)]\n",
    "# init the environment\n",
    "env = GamblingRoom(k=num_action)\n",
    "for epsilon in epsilons:\n",
    "    best_action = np.argmax([slot_machine.mu for slot_machine in env.r_machines]) # best action in a gambling machine\n",
    "\n",
    "    # delete the wrap\n",
    "    env = env.unwrapped\n",
    "\n",
    "    environment_avg = []\n",
    "    best_picked_run = []\n",
    "\n",
    "    #for environment in range(3):\n",
    "    avg_rewards= []\n",
    "    most_frequent = [0 for x in range(num_action)]\n",
    "\n",
    "    # run multiple simulations\n",
    "    for i_run in range(num_runs): \n",
    "        rewards= []\n",
    "        best_picked_step = []\n",
    "        # init the epsilon-greedy RL agent \n",
    "        agent = EplisonGreedyAgent(num_action, epsilon)\n",
    "        # in each simulation run, loop the action selection\n",
    "        # save the result variables you need\n",
    "        for step in range(steps):\n",
    "            action = agent.select_action()\n",
    "            most_frequent[action] =  most_frequent[action] +1\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            agent.update_parameters(action, reward)\n",
    "            condition = action ==best_action\n",
    "            best_picked_step.append(condition)\n",
    "\n",
    "        best_picked_run.append(best_picked_step)\n",
    "\n",
    "        avg_rewards.append(rewards)\n",
    "       # optimal_action = np.argmax(agent.Qvalues)\n",
    "\n",
    "    #environment_avg.append(avg_rewards)\n",
    "\n",
    "    #average the array\n",
    "    ave_array = np.mean(np.array(avg_rewards), axis=0)\n",
    "\n",
    "    # The percentage of times that the optimal action was picked at each episode over the whole runs\n",
    "    best_picked_run = np.mean(best_picked_run, axis = 0)\n",
    "    epsilons_list.append((epsilon,ave_array,most_frequent,best_action,best_picked_run))\n",
    "    epsilons_list_env2 = epsilons_list\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30,30))\n",
    "pp = PdfPages('Save multiple plots as PDF.pdf')\n",
    "\n",
    "for axis, environment, n_env in zip(ax, epsilons_list_env3, range(1, len(env_s_seed_3)+1)):\n",
    "    # Average reward graph\n",
    "    for graph in environment:\n",
    "        axis[0].plot(graph[-1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "    axis[0].set_ylabel(f\"Optimal Action % - Environment {n_env}\")\n",
    "    axis[0].set_xlabel(\"Steps\")\n",
    "    axis[0].legend(loc=\"lower right\")\n",
    "    # Optimal action graph\n",
    "    for graph in environment:\n",
    "        axis[1].plot(graph[1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "    axis[1].set_ylabel(f\"Average reward - Environment {n_env}\")\n",
    "    axis[1].set_xlabel(\"Steps\")\n",
    "    axis[1].legend(loc=\"lower right\")\n",
    "\n",
    "ax[0,0].set_title(\"Optimal Action graphs per environment\")\n",
    "ax[0,1].set_title(\"Average rewards graphs per environment\")\n",
    "\n",
    "plt.suptitle(\"EXPLANATORY GRAPHS PER ENVIRONMENT\", fontsize= 30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENVIRONMENT Combined\n",
    "# Testbed - variables\n",
    "num_action = 10\n",
    "num_seed = [3]\n",
    "num_runs = 100  # number of simulation runs\n",
    "steps = 300  # number of steps in each run\n",
    "epsilons= list(np.linspace(0,0.2,8)) + [1]\n",
    "env_s_seed_3 = []\n",
    "\n",
    "# set up the random seed\n",
    "np.random.seed(num_seed)\n",
    "for seed in num_seed:\n",
    "    epsilons_list = []\n",
    "    total_actions= [0 for x in range(num_action)]\n",
    "    # init the environment\n",
    "    env = GamblingRoom(k=num_action)\n",
    "    for epsilon in epsilons:\n",
    "        best_action = np.argmax([slot_machine.mu for slot_machine in env.r_machines]) # best action in a gambling machine\n",
    "\n",
    "        # delete the wrap\n",
    "        env = env.unwrapped\n",
    "\n",
    "        environment_avg = []\n",
    "        best_picked_run = []\n",
    "\n",
    "        avg_rewards= []\n",
    "        most_frequent = [0 for x in range(num_action)]\n",
    "\n",
    "        # run multiple simulations\n",
    "        for i_run in range(num_runs): \n",
    "            rewards= []\n",
    "            best_picked_step = []\n",
    "            # init the epsilon-greedy RL agent \n",
    "            agent = EplisonGreedyAgent(num_action, epsilon)\n",
    "            # in each simulation run, loop the action selection\n",
    "            # save the result variables you need\n",
    "            for step in range(steps):\n",
    "                action = agent.select_action()\n",
    "                most_frequent[action] =  most_frequent[action] +1\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                agent.update_parameters(action, reward)\n",
    "                condition = action ==best_action\n",
    "                best_picked_step.append(condition)\n",
    "\n",
    "            best_picked_run.append(best_picked_step)\n",
    "\n",
    "            avg_rewards.append(rewards)\n",
    "        # optimal_action = np.argmax(agent.Qvalues)\n",
    "\n",
    "        #environment_avg.append(avg_rewards)\n",
    "\n",
    "        #average the array\n",
    "        ave_array = np.mean(np.array(avg_rewards), axis=0)\n",
    "\n",
    "        # The percentage of times that the optimal action was picked at each episode over the whole runs\n",
    "        best_picked_run = np.mean(best_picked_run, axis = 0)\n",
    "        epsilons_list.append((epsilon,ave_array,most_frequent,best_action,best_picked_run))\n",
    "        epsilons_list_env3 = epsilons_list\n",
    "        env_s_seed_3.append(epsilons_list)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import rcParams\n",
    "#matplotlib.use(\"cairo\")\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idan Explarimentation\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(30,10), squeeze=False)\n",
    "pp = PdfPages('Save multiple plots as PDF.pdf')\n",
    "\n",
    "for axis, environment, n_env in zip(ax, env_s_seed_3, range(1, len(env_s_seed_3)+1)):\n",
    "    # Average reward graph\n",
    "    for graph in environment:\n",
    "        axis[0].plot(graph[-1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "    axis[0].set_ylabel(f\"Optimal Action % - Environment {n_env}\")\n",
    "    axis[0].set_xlabel(\"Steps\")\n",
    "    axis[0].legend(loc=\"lower right\")\n",
    "    # Optimal action graph\n",
    "    for graph in environment:\n",
    "        axis[1].plot(graph[1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "    axis[1].set_ylabel(f\"Average reward - Environment {n_env}\")\n",
    "    axis[1].set_xlabel(\"Steps\")\n",
    "    axis[1].legend(loc=\"lower right\")\n",
    "\n",
    "ax[0,0].set_title(\"Optimal Action graphs per environment\")\n",
    "ax[0,1].set_title(\"Average rewards graphs per environment\")\n",
    "\n",
    "plt.suptitle(\"EXPLANATORY GRAPHS PER ENVIRONMENT\", fontsize= 30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = [epsilons_list_env1, epsilons_list_env2, epsilons_list_env3]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30,30))\n",
    "\n",
    "for axis, environment, n_env in zip(ax, env_info, range(1, len(env_info)+1)):\n",
    "    # Average reward graph\n",
    "    for graph in environment:\n",
    "        axis[0].plot(graph[-1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "    axis[0].set_ylabel(f\"Optimal Action % - Environment {n_env}\")\n",
    "    axis[0].set_xlabel(\"Steps\")\n",
    "    axis[0].legend(loc=\"lower right\")\n",
    "    # Optimal action graph\n",
    "    for graph in environment:\n",
    "        axis[1].plot(graph[1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "    axis[1].set_ylabel(f\"Average reward - Environment {n_env}\")\n",
    "    axis[1].set_xlabel(\"Steps\")\n",
    "    axis[1].legend(loc=\"lower right\")\n",
    "\n",
    "ax[0,0].set_title(\"Optimal Action graphs per environment\")\n",
    "ax[0,1].set_title(\"Average rewards graphs per environment\")\n",
    "\n",
    "plt.suptitle(\"EXPLANATORY GRAPHS PER ENVIRONMENT\", fontsize= 30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# fig = plt.figure(figsize=(20,10))\n",
    "# for graph in epsilons_list:\n",
    "#     plt.plot(graph[-1], label=f\"epsilon = {round(graph[0], 2)}\")\n",
    "\n",
    "\n",
    "# plt.ylabel(\"Optimal Action %\")\n",
    "# plt.xlabel(\"Steps\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environments = []\n",
    "num_action = 10\n",
    "for i in range(1, 4):\n",
    "    np.random.seed(i)\n",
    "    environments.append(GamblingRoom(k=num_action))\n",
    "\n",
    "print([f\"{machine.mu}, {machine.sigma}\" for machine in environments[0].r_machines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(nrows=len(environments), ncols=1, figsize=(30,30))\n",
    "for axis, env_obj, n_env in zip(ax, environments, range(1, len(environments)+1)):\n",
    "    distribution_data = [np.random.normal(loc=machine.mu, scale=machine.sigma, size=10000) for machine in env_obj.r_machines]\n",
    "    sns.violinplot(data =distribution_data, ax = axis)\n",
    "    axis.set_title(f\"Environment {n_env}\\n\")\n",
    "    axis.set_xlabel(\"Gambling Machine\\n\")\n",
    "    axis.set_xticklabels(range(1,11))\n",
    "    axis.set_ylabel(\"Reward\\n\")\n",
    "plt.suptitle(\"Distribution graphs per environment\\n\\n\", fontsize= 30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59506e3267c903511e7697cc8f12bba02951348adc30c0306c51977b1a6c809f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
